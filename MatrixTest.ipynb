{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import laspy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import jit\n",
    "from numba.core import types\n",
    "from numba.typed import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CONFIGURATION ---\n",
    "# Set all simulation parameters in this section.\n",
    "\n",
    "# 1. Input Data and Scene Definition\n",
    "LIDAR_FILE_PATH = 'data/houselas_re_veg_2.las'\n",
    "OUTPUT_DIRECTORY = 'results/shadow_matrix_results'\n",
    "OUTPUT_FILENAME = 'shadow_attenuation_matrix_conecasting.csv'\n",
    "BOUNDING_BOX = None\n",
    "\n",
    "# ASPRS Standard Classification Codes\n",
    "# Including multiple vegetation classes (3: Low, 4: Medium, 5: High)\n",
    "RELEVANT_CLASSES = {2, 3, 4, 5, 6}\n",
    "GROUND_CLASS_CODE = 2\n",
    "BUILDING_CLASS_CODE = 6\n",
    "VEGETATION_CLASS_CODES = {3, 4, 5}\n",
    "\n",
    "# Priority for voxel classification (higher number = higher priority)\n",
    "# High vegetation (5) has priority over medium (4), etc.\n",
    "CLASS_PRIORITY = {6: 4, 5: 3, 4: 2, 3: 1, 2: 0, 0: -1}\n",
    "\n",
    "# 2. Voxelization Parameters\n",
    "VOXEL_SIZE = 0.5\n",
    "\n",
    "# 3. Solar Position & Cone-Casting Simulation Parameters\n",
    "AZIMUTH_STEPS = 360  # 1-degree steps\n",
    "ELEVATION_STEPS = 91 # 1-degree steps (0-90 inclusive)\n",
    "\n",
    "# --- NEW: Cone-Casting Parameters ---\n",
    "# The sun's angular radius is approx 0.265 degrees.\n",
    "SOLAR_ANGULAR_RADIUS_DEG = 0.265\n",
    "# Number of rays to cast to approximate the solar disk. More rays = more\n",
    "# accurate penumbra but longer computation time.\n",
    "NUM_RAYS_PER_CONE = 16\n",
    "\n",
    "# 4. Ray-Casting and Attenuation Parameters\n",
    "# --- UPDATED: Class-specific extinction coefficients ---\n",
    "# Assign a different base extinction coefficient (k) to each vegetation class.\n",
    "# These values may need to be calibrated for specific vegetation types.\n",
    "VEGETATION_EXTINCTION_COEFFICIENTS = {\n",
    "    3: 0.7,  # k for Low Vegetation\n",
    "    4: 0.6,  # k for Medium Vegetation\n",
    "    5: 0.4   # k for High Vegetation\n",
    "}\n",
    "\n",
    "# 5. Performance Parameters\n",
    "NUM_CORES = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODULE 1: DATA INGESTOR & PREPARER ---\n",
    "\n",
    "def load_and_prepare_lidar(file_path, bounding_box=None, relevant_classes=None):\n",
    "    \"\"\"\n",
    "    Reads a LAS/LAZ file, filters points by classification and bounding box.\n",
    "    \"\"\"\n",
    "    print(f\"Loading LiDAR data from {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None, None\n",
    "    try:\n",
    "        las = laspy.read(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    points_xyz = np.vstack((las.x, las.y, las.z)).transpose()\n",
    "    classifications = np.array(las.classification)\n",
    "\n",
    "    mask = np.ones(len(points_xyz), dtype=bool)\n",
    "    if bounding_box:\n",
    "        mask &= (\n",
    "            (points_xyz[:, 0] >= bounding_box['X_MIN']) & (points_xyz[:, 0] < bounding_box['X_MAX']) &\n",
    "            (points_xyz[:, 1] >= bounding_box['Y_MIN']) & (points_xyz[:, 1] < bounding_box['Y_MAX'])\n",
    "        )\n",
    "    if relevant_classes:\n",
    "        mask &= np.isin(classifications, list(relevant_classes))\n",
    "\n",
    "    filtered_points = points_xyz[mask]\n",
    "    filtered_classifications = classifications[mask]\n",
    "\n",
    "    if len(filtered_points) == 0:\n",
    "        print(\"Warning: No points remaining after filtering.\")\n",
    "        return None, None\n",
    "    print(f\"Data loaded and filtered. {len(filtered_points)} points remaining.\")\n",
    "    return filtered_points, filtered_classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODULE 2: VOXELIZER ---\n",
    "\n",
    "def voxelize_scene(points, classifications, voxel_size):\n",
    "    \"\"\"\n",
    "    Converts a point cloud into classification and density voxel grids.\n",
    "    \"\"\"\n",
    "    if points is None or len(points) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(\"Voxelizing the scene...\")\n",
    "    scene_min = np.min(points, axis=0)\n",
    "    scene_max = np.max(points, axis=0)\n",
    "    grid_dims = np.ceil((scene_max - scene_min) / voxel_size).astype(int)\n",
    "    print(f\"Voxel grid dimensions: {grid_dims}\")\n",
    "\n",
    "    classification_grid = np.zeros(grid_dims, dtype=np.int8)\n",
    "    density_grid = np.zeros(grid_dims, dtype=np.float32)\n",
    "\n",
    "    voxel_indices = np.floor((points - scene_min) / voxel_size).astype(int)\n",
    "    for i in range(3):\n",
    "        voxel_indices[:, i] = np.clip(voxel_indices[:, i], 0, grid_dims[i] - 1)\n",
    "\n",
    "    print(\"Populating classification and density grids...\")\n",
    "    for i in range(len(points)):\n",
    "        idx = tuple(voxel_indices[i])\n",
    "        current_class = classifications[i]\n",
    "        if CLASS_PRIORITY.get(current_class, -1) > CLASS_PRIORITY.get(classification_grid[idx], -1):\n",
    "            classification_grid[idx] = current_class\n",
    "        if current_class in VEGETATION_CLASS_CODES:\n",
    "            density_grid[idx] += 1\n",
    "\n",
    "    voxel_volume = voxel_size ** 3\n",
    "    vegetation_voxels = np.isin(classification_grid, list(VEGETATION_CLASS_CODES))\n",
    "    density_grid[vegetation_voxels] /= voxel_volume\n",
    "    \n",
    "    print(\"Voxelization complete.\")\n",
    "    return classification_grid, density_grid, scene_min, grid_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODULE 3: RAY-CASTING & CONE-CASTING ENGINE ---\n",
    "\n",
    "def generate_cone_vectors(center_direction, radius_rad, num_samples):\n",
    "    \"\"\"\n",
    "    Generates a set of vectors distributed within a cone around a center direction.\n",
    "    \"\"\"\n",
    "    # Create a basis (a coordinate system) aligned with the center_direction\n",
    "    if np.allclose(np.abs(center_direction), [0, 0, 1]):\n",
    "        # Handle case where direction is along Z-axis\n",
    "        v_up = np.array([0, 1, 0])\n",
    "    else:\n",
    "        v_up = np.array([0, 0, 1])\n",
    "    \n",
    "    u = np.cross(v_up, center_direction)\n",
    "    u /= np.linalg.norm(u)\n",
    "    v = np.cross(center_direction, u)\n",
    "\n",
    "    cone_vectors = []\n",
    "    # Use stratified sampling for better distribution\n",
    "    for i in range(num_samples):\n",
    "        # Sample radius and angle to get points on a disk\n",
    "        r = radius_rad * np.sqrt((i + 0.5) / num_samples)\n",
    "        theta = 2 * np.pi * 0.61803398875 * i # Golden angle for good distribution\n",
    "\n",
    "        # Map disk point to 3D offset and add to center direction\n",
    "        offset_vec = r * (np.cos(theta) * u + np.sin(theta) * v)\n",
    "        new_vec = center_direction + offset_vec\n",
    "        new_vec /= np.linalg.norm(new_vec) # Re-normalize\n",
    "        cone_vectors.append(new_vec)\n",
    "        \n",
    "    return cone_vectors\n",
    "\n",
    "@jit(nopython=True)\n",
    "def trace_ray_fast(ray_origin, ray_direction, scene_min, voxel_size, grid_dims):\n",
    "    \"\"\"\n",
    "    An efficient voxel traversal algorithm (Amanatides-Woo).\n",
    "    \"\"\"\n",
    "    ray_pos = (ray_origin - scene_min) / voxel_size\n",
    "    ix, iy, iz = int(ray_pos[0]), int(ray_pos[1]), int(ray_pos[2])\n",
    "    step_x = 1 if ray_direction[0] >= 0 else -1\n",
    "    step_y = 1 if ray_direction[1] >= 0 else -1\n",
    "    step_z = 1 if ray_direction[2] >= 0 else -1\n",
    "    \n",
    "    next_voxel_boundary_x = (ix + (step_x > 0)) * voxel_size + scene_min[0]\n",
    "    next_voxel_boundary_y = (iy + (step_y > 0)) * voxel_size + scene_min[1]\n",
    "    next_voxel_boundary_z = (iz + (step_z > 0)) * voxel_size + scene_min[2]\n",
    "    \n",
    "    t_max_x = (next_voxel_boundary_x - ray_origin[0]) / ray_direction[0] if ray_direction[0] != 0 else np.inf\n",
    "    t_max_y = (next_voxel_boundary_y - ray_origin[1]) / ray_direction[1] if ray_direction[1] != 0 else np.inf\n",
    "    t_max_z = (next_voxel_boundary_z - ray_origin[2]) / ray_direction[2] if ray_direction[2] != 0 else np.inf\n",
    "    \n",
    "    t_delta_x = voxel_size / abs(ray_direction[0]) if ray_direction[0] != 0 else np.inf\n",
    "    t_delta_y = voxel_size / abs(ray_direction[1]) if ray_direction[1] != 0 else np.inf\n",
    "    t_delta_z = voxel_size / abs(ray_direction[2]) if ray_direction[2] != 0 else np.inf\n",
    "\n",
    "    while True:\n",
    "        if not (0 <= ix < grid_dims[0] and 0 <= iy < grid_dims[1] and 0 <= iz < grid_dims[2]):\n",
    "            break\n",
    "        yield (ix, iy, iz)\n",
    "        if t_max_x < t_max_y:\n",
    "            if t_max_x < t_max_z:\n",
    "                ix += step_x; t_max_x += t_delta_x\n",
    "            else:\n",
    "                iz += step_z; t_max_z += t_delta_z\n",
    "        else:\n",
    "            if t_max_y < t_max_z:\n",
    "                iy += step_y; t_max_y += t_delta_y\n",
    "            else:\n",
    "                iz += step_z; t_max_z += t_delta_z\n",
    "\n",
    "@jit(nopython=True)\n",
    "def calculate_transmittance(voxel_path_generator, classification_grid, density_grid, voxel_size, k_coeffs):\n",
    "    \"\"\"\n",
    "    Calculates the final transmittance for a single ray path, using class-specific\n",
    "    extinction coefficients for different vegetation types.\n",
    "    \"\"\"\n",
    "    transmittance = 1.0\n",
    "    path_length_in_voxel = voxel_size\n",
    "    for ix, iy, iz in voxel_path_generator:\n",
    "        voxel_class = classification_grid[ix, iy, iz]\n",
    "        if voxel_class == BUILDING_CLASS_CODE: return 0.0\n",
    "        \n",
    "        # Check if the voxel is any type of vegetation\n",
    "        if voxel_class in (3, 4, 5):\n",
    "            k_base = k_coeffs.get(voxel_class, 0.0) # Look up the correct k\n",
    "            if k_base > 0:\n",
    "                density = density_grid[ix, iy, iz]\n",
    "                if density > 0:\n",
    "                    k = k_base * density\n",
    "                    transmittance *= math.exp(-k * path_length_in_voxel)\n",
    "        \n",
    "        if transmittance < 1e-6: return 0.0\n",
    "    return transmittance\n",
    "\n",
    "def run_single_ray_simulation(args):\n",
    "    \"\"\"\n",
    "    Worker function to trace a single ray. For use with multiprocessing.\n",
    "    \"\"\"\n",
    "    ray_direction, analysis_point, grid_dims, sim_data = args\n",
    "    classification_grid = sim_data['classification_grid']\n",
    "    density_grid = sim_data['density_grid']\n",
    "    voxel_size = sim_data['voxel_size']\n",
    "    k_coeffs = sim_data['k_coeffs']\n",
    "    scene_min = sim_data['scene_min']\n",
    "    \n",
    "    voxel_path_generator = trace_ray_fast(analysis_point, ray_direction, scene_min, voxel_size, grid_dims)\n",
    "    return calculate_transmittance(voxel_path_generator, classification_grid, density_grid, voxel_size, k_coeffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN EXECUTION ---\n",
    "def run_all():\n",
    "# if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Load and Prepare Data\n",
    "    points, classifications = load_and_prepare_lidar(\n",
    "        LIDAR_FILE_PATH, BOUNDING_BOX, RELEVANT_CLASSES\n",
    "    )\n",
    "    if points is None: exit()\n",
    "\n",
    "    # 2. Voxelize Scene\n",
    "    classification_grid, density_grid, scene_min, grid_dims = voxelize_scene(\n",
    "        points, classifications, VOXEL_SIZE\n",
    "    )\n",
    "    if classification_grid is None: exit()\n",
    "    scene_max = scene_min + grid_dims * VOXEL_SIZE\n",
    "\n",
    "    # 3. Define the single analysis point at the center of the scene\n",
    "    scene_center_x = (scene_min[0] + scene_max[0]) / 2\n",
    "    scene_center_y = (scene_min[1] + scene_max[1]) / 2\n",
    "\n",
    "    ground_points_all = points[classifications == GROUND_CLASS_CODE]\n",
    "    if len(ground_points_all) > 0:\n",
    "        search_radius = 5.0\n",
    "        center_points_mask = (\n",
    "            (ground_points_all[:, 0] > scene_center_x - search_radius) &\n",
    "            (ground_points_all[:, 0] < scene_center_x + search_radius) &\n",
    "            (ground_points_all[:, 1] > scene_center_y - search_radius) &\n",
    "            (ground_points_all[:, 1] < scene_center_y + search_radius)\n",
    "        )\n",
    "        ground_points_near_center = ground_points_all[center_points_mask]\n",
    "        if len(ground_points_near_center) > 0:\n",
    "            ground_z = np.min(ground_points_near_center[:, 2]) + 0.01\n",
    "        else:\n",
    "            print(\"Warning: No ground points near center. Using scene minimum Z.\")\n",
    "            ground_z = scene_min[2] + 0.01\n",
    "    else:\n",
    "        print(\"Warning: No ground points in dataset. Using scene minimum Z.\")\n",
    "        ground_z = scene_min[2] + 0.01\n",
    "\n",
    "    analysis_point = np.array([scene_center_x, scene_center_y, ground_z])\n",
    "    print(f\"Analysis point set to scene center: {analysis_point}\")\n",
    "    \n",
    "    # # Prepare a Numba-compatible dictionary for the JIT-compiled function\n",
    "    # k_coeffs_numba = Dict.empty(key_type=types.int64, value_type=types.float64)\n",
    "    # for k, v in VEGETATION_EXTINCTION_COEFFICIENTS.items():\n",
    "    #     k_coeffs_numba[k] = v\n",
    "\n",
    "    # simulation_data = {\n",
    "    #     'classification_grid': classification_grid,\n",
    "    #     'density_grid': density_grid,\n",
    "    #     'voxel_size': VOXEL_SIZE,\n",
    "    #     'k_coeffs': k_coeffs_numba,\n",
    "    #     'scene_min': scene_min\n",
    "    # }\n",
    "\n",
    "    # --- FIX: Pass the standard python dictionary ---\n",
    "    simulation_data = {\n",
    "        'classification_grid': classification_grid,\n",
    "        'density_grid': density_grid,\n",
    "        'voxel_size': VOXEL_SIZE,\n",
    "        'k_coeffs': VEGETATION_EXTINCTION_COEFFICIENTS, # Pass the standard dict\n",
    "        'scene_min': scene_min\n",
    "    }\n",
    "\n",
    "    # 4. Define Solar Angles and Run Simulation Loop\n",
    "    azimuths = np.linspace(0, 2 * np.pi, AZIMUTH_STEPS, endpoint=False)\n",
    "    elevations = np.linspace(0, np.pi / 2, ELEVATION_STEPS, endpoint=True)\n",
    "    solar_radius_rad = np.deg2rad(SOLAR_ANGULAR_RADIUS_DEG)\n",
    "    \n",
    "    simulation_results = []\n",
    "    \n",
    "    total_directions = len(azimuths) * (len(elevations) - 1)\n",
    "    print(f\"\\n--- Starting Cone-Casting Simulation for Center Point ---\")\n",
    "    print(f\"Casting {NUM_RAYS_PER_CONE} rays per cone for each of {total_directions} solar positions...\")\n",
    "    \n",
    "    current_direction = 0\n",
    "    with Pool(processes=NUM_CORES) as pool:\n",
    "        for el in elevations:\n",
    "            if el < 0.001: continue\n",
    "            \n",
    "            for az in azimuths:\n",
    "                current_direction += 1\n",
    "                if current_direction % 500 == 0:\n",
    "                     print(f\"Processing direction {current_direction} of {total_directions} (Az: {np.rad2deg(az):.1f}°, El: {np.rad2deg(el):.1f}°)...\")\n",
    "                \n",
    "                center_ray_direction = np.array([np.cos(el) * np.sin(az), np.cos(el) * np.cos(az), np.sin(el)])\n",
    "                \n",
    "                cone_ray_vectors = generate_cone_vectors(center_ray_direction, solar_radius_rad, NUM_RAYS_PER_CONE)\n",
    "                \n",
    "                task_args = [(vec, analysis_point, grid_dims, simulation_data) for vec in cone_ray_vectors]\n",
    "                \n",
    "                transmittances = pool.map(run_single_ray_simulation, task_args)\n",
    "                \n",
    "                avg_transmittance = np.mean(transmittances)\n",
    "                \n",
    "                simulation_results.append({\n",
    "                    'azimuth': az, 'elevation': el, 'transmittance': avg_transmittance\n",
    "                })\n",
    "\n",
    "    # 5. Format and Save Final Attenuation Matrix\n",
    "    print(\"\\n--- Aggregating Results into CSV Matrix ---\")\n",
    "    if not simulation_results:\n",
    "        print(\"No results to save. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    df = pd.DataFrame(simulation_results)\n",
    "    df['azimuth_deg'] = np.round(np.rad2deg(df['azimuth'])).astype(int)\n",
    "    df['elevation_deg'] = np.round(np.rad2deg(df['elevation'])).astype(int)\n",
    "    \n",
    "    shadow_matrix_df = df.pivot_table(\n",
    "        index='elevation_deg', columns='azimuth_deg', values='transmittance'\n",
    "    )\n",
    "    \n",
    "    shadow_matrix_df.index = [f\"Altitude_{i}\" for i in shadow_matrix_df.index]\n",
    "    shadow_matrix_df.columns = [f\"Azimuth_{c}\" for c in shadow_matrix_df.columns]\n",
    "    \n",
    "    shadow_matrix_df = shadow_matrix_df.sort_index(key=lambda x: np.array([int(i.split('_')[1]) for i in x]))\n",
    "    shadow_matrix_df = shadow_matrix_df.sort_index(axis=1, key=lambda x: np.array([int(i.split('_')[1]) for i in x]))\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIRECTORY):\n",
    "        os.makedirs(OUTPUT_DIRECTORY)\n",
    "        \n",
    "    output_path = os.path.join(OUTPUT_DIRECTORY, OUTPUT_FILENAME)\n",
    "    shadow_matrix_df.to_csv(output_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- Simulation Finished ---\")\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Saved shadow attenuation matrix to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_all()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
