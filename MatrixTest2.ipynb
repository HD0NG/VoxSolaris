{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxel-Based Cone-Casting Shadow Attenuation Matrix Generator\n",
    "#\n",
    "# This script implements an advanced framework to simulate light attenuation\n",
    "# using a cone-casting method to account for the sun's angular diameter,\n",
    "# producing more realistic soft shadows (penumbra).\n",
    "#\n",
    "# This version is configured to analyze a single point at the center of the LiDAR scene\n",
    "# and supports multiple vegetation classes with distinct attenuation coefficients.\n",
    "#\n",
    "# NOTE: This version has been simplified to run sequentially without Numba or\n",
    "# multiprocessing to ensure stability and ease of debugging. Performance will be\n",
    "# significantly lower than optimized versions.\n",
    "#\n",
    "# The pipeline is as follows:\n",
    "# 1. Data Ingestion: Reads and filters classified LiDAR data (.las/.laz).\n",
    "# 2. Voxelization: Converts the point cloud into a 3D voxel grid.\n",
    "# 3. Iterative Cone-Casting: For each solar position, it generates a cone of\n",
    "#    rays representing the sun's disk.\n",
    "# 4. Attenuation Modeling: For each ray in the cone, it calculates transmittance\n",
    "#    based on binary occlusion (buildings) or the Beer-Lambert law (vegetation),\n",
    "#    using class-specific extinction coefficients.\n",
    "# 5. Output Generation: The final transmittance for a solar position is the\n",
    "#    average of all rays in the cone. These values are aggregated into a\n",
    "#    2D matrix and saved as a CSV file.\n",
    "#\n",
    "# Key Libraries:\n",
    "# - laspy: For reading LiDAR files.\n",
    "# - numpy: For numerical operations and data structures.\n",
    "# - pandas: For creating and exporting the final CSV data matrix.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import laspy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Set all simulation parameters in this section.\n",
    "\n",
    "# 1. Input Data and Scene Definition\n",
    "LIDAR_FILE_PATH = 'data/houselas_re_veg_2.las'\n",
    "OUTPUT_DIRECTORY = 'results/shadow_matrix_results'\n",
    "OUTPUT_FILENAME = 'shadow_attenuation_matrix_conecasting.csv'\n",
    "BOUNDING_BOX = None\n",
    "\n",
    "# ASPRS Standard Classification Codes\n",
    "# Including multiple vegetation classes (3: Low, 4: Medium, 5: High)\n",
    "RELEVANT_CLASSES = {2, 3, 4, 5, 6}\n",
    "GROUND_CLASS_CODE = 2\n",
    "BUILDING_CLASS_CODE = 6\n",
    "VEGETATION_CLASS_CODES = {3, 4, 5}\n",
    "\n",
    "# Priority for voxel classification (higher number = higher priority)\n",
    "# High vegetation (5) has priority over medium (4), etc.\n",
    "CLASS_PRIORITY = {6: 4, 5: 3, 4: 2, 3: 1, 2: 0, 0: -1}\n",
    "\n",
    "# 2. Voxelization Parameters\n",
    "VOXEL_SIZE = 0.5\n",
    "\n",
    "# 3. Solar Position & Cone-Casting Simulation Parameters\n",
    "AZIMUTH_STEPS = 360  # 1-degree steps\n",
    "ELEVATION_STEPS = 91 # 1-degree steps (0-90 inclusive)\n",
    "\n",
    "# --- NEW: Cone-Casting Parameters ---\n",
    "# The sun's angular radius is approx 0.265 degrees.\n",
    "SOLAR_ANGULAR_RADIUS_DEG = 0.265\n",
    "# Number of rays to cast to approximate the solar disk. More rays = more\n",
    "# accurate penumbra but longer computation time.\n",
    "NUM_RAYS_PER_CONE = 16\n",
    "\n",
    "# 4. Ray-Casting and Attenuation Parameters\n",
    "# --- UPDATED: Class-specific extinction coefficients ---\n",
    "# Assign a different base extinction coefficient (k) to each vegetation class.\n",
    "# These values may need to be calibrated for specific vegetation types.\n",
    "VEGETATION_EXTINCTION_COEFFICIENTS = {\n",
    "    3: 0.7,  # k for Low Vegetation\n",
    "    4: 0.5,  # k for Medium Vegetation\n",
    "    5: 0.3   # k for High Vegetation\n",
    "}\n",
    "\n",
    "\n",
    "# --- MODULE 1: DATA INGESTOR & PREPARER ---\n",
    "\n",
    "def load_and_prepare_lidar(file_path, bounding_box=None, relevant_classes=None):\n",
    "    \"\"\"\n",
    "    Reads a LAS/LAZ file, filters points by classification and bounding box.\n",
    "    \"\"\"\n",
    "    print(f\"Loading LiDAR data from {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None, None\n",
    "    try:\n",
    "        las = laspy.read(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    points_xyz = np.vstack((las.x, las.y, las.z)).transpose()\n",
    "    classifications = np.array(las.classification)\n",
    "\n",
    "    mask = np.ones(len(points_xyz), dtype=bool)\n",
    "    if bounding_box:\n",
    "        mask &= (\n",
    "            (points_xyz[:, 0] >= bounding_box['X_MIN']) & (points_xyz[:, 0] < bounding_box['X_MAX']) &\n",
    "            (points_xyz[:, 1] >= bounding_box['Y_MIN']) & (points_xyz[:, 1] < bounding_box['Y_MAX'])\n",
    "        )\n",
    "    if relevant_classes:\n",
    "        mask &= np.isin(classifications, list(relevant_classes))\n",
    "\n",
    "    filtered_points = points_xyz[mask]\n",
    "    filtered_classifications = classifications[mask]\n",
    "\n",
    "    if len(filtered_points) == 0:\n",
    "        print(\"Warning: No points remaining after filtering.\")\n",
    "        return None, None\n",
    "    print(f\"Data loaded and filtered. {len(filtered_points)} points remaining.\")\n",
    "    return filtered_points, filtered_classifications\n",
    "\n",
    "\n",
    "# --- MODULE 2: VOXELIZER ---\n",
    "\n",
    "def voxelize_scene(points, classifications, voxel_size):\n",
    "    \"\"\"\n",
    "    Converts a point cloud into classification and density voxel grids.\n",
    "    \"\"\"\n",
    "    if points is None or len(points) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(\"Voxelizing the scene...\")\n",
    "    scene_min = np.min(points, axis=0)\n",
    "    scene_max = np.max(points, axis=0)\n",
    "    grid_dims = np.ceil((scene_max - scene_min) / voxel_size).astype(int)\n",
    "    print(f\"Voxel grid dimensions: {grid_dims}\")\n",
    "\n",
    "    classification_grid = np.zeros(grid_dims, dtype=np.int8)\n",
    "    density_grid = np.zeros(grid_dims, dtype=np.float32)\n",
    "\n",
    "    voxel_indices = np.floor((points - scene_min) / voxel_size).astype(int)\n",
    "    for i in range(3):\n",
    "        voxel_indices[:, i] = np.clip(voxel_indices[:, i], 0, grid_dims[i] - 1)\n",
    "\n",
    "    print(\"Populating classification and density grids...\")\n",
    "    for i in range(len(points)):\n",
    "        idx = tuple(voxel_indices[i])\n",
    "        current_class = classifications[i]\n",
    "        if CLASS_PRIORITY.get(current_class, -1) > CLASS_PRIORITY.get(classification_grid[idx], -1):\n",
    "            classification_grid[idx] = current_class\n",
    "        if current_class in VEGETATION_CLASS_CODES:\n",
    "            density_grid[idx] += 1\n",
    "\n",
    "    voxel_volume = voxel_size ** 3\n",
    "    vegetation_voxels = np.isin(classification_grid, list(VEGETATION_CLASS_CODES))\n",
    "    density_grid[vegetation_voxels] /= voxel_volume\n",
    "    \n",
    "    print(\"Voxelization complete.\")\n",
    "    return classification_grid, density_grid, scene_min, grid_dims\n",
    "\n",
    "\n",
    "# --- MODULE 3: RAY-CASTING & CONE-CASTING ENGINE ---\n",
    "\n",
    "def generate_cone_vectors(center_direction, radius_rad, num_samples):\n",
    "    \"\"\"\n",
    "    Generates a set of vectors distributed within a cone around a center direction.\n",
    "    \"\"\"\n",
    "    # Create a basis (a coordinate system) aligned with the center_direction\n",
    "    if np.allclose(np.abs(center_direction), [0, 0, 1]):\n",
    "        # Handle case where direction is along Z-axis\n",
    "        v_up = np.array([0, 1, 0])\n",
    "    else:\n",
    "        v_up = np.array([0, 0, 1])\n",
    "    \n",
    "    u = np.cross(v_up, center_direction)\n",
    "    u /= np.linalg.norm(u)\n",
    "    v = np.cross(center_direction, u)\n",
    "\n",
    "    cone_vectors = []\n",
    "    # Use stratified sampling for better distribution\n",
    "    for i in range(num_samples):\n",
    "        # Sample radius and angle to get points on a disk\n",
    "        r = radius_rad * np.sqrt((i + 0.5) / num_samples)\n",
    "        theta = 2 * np.pi * 0.61803398875 * i # Golden angle for good distribution\n",
    "\n",
    "        # Map disk point to 3D offset and add to center direction\n",
    "        offset_vec = r * (np.cos(theta) * u + np.sin(theta) * v)\n",
    "        new_vec = center_direction + offset_vec\n",
    "        new_vec /= np.linalg.norm(new_vec) # Re-normalize\n",
    "        cone_vectors.append(new_vec)\n",
    "        \n",
    "    return cone_vectors\n",
    "\n",
    "def trace_ray_fast(ray_origin, ray_direction, scene_min, voxel_size, grid_dims):\n",
    "    \"\"\"\n",
    "    An efficient voxel traversal algorithm (Amanatides-Woo).\n",
    "    \"\"\"\n",
    "    ray_pos = (ray_origin - scene_min) / voxel_size\n",
    "    ix, iy, iz = int(ray_pos[0]), int(ray_pos[1]), int(ray_pos[2])\n",
    "    step_x = 1 if ray_direction[0] >= 0 else -1\n",
    "    step_y = 1 if ray_direction[1] >= 0 else -1\n",
    "    step_z = 1 if ray_direction[2] >= 0 else -1\n",
    "    \n",
    "    next_voxel_boundary_x = (ix + (step_x > 0)) * voxel_size + scene_min[0]\n",
    "    next_voxel_boundary_y = (iy + (step_y > 0)) * voxel_size + scene_min[1]\n",
    "    next_voxel_boundary_z = (iz + (step_z > 0)) * voxel_size + scene_min[2]\n",
    "    \n",
    "    t_max_x = (next_voxel_boundary_x - ray_origin[0]) / ray_direction[0] if ray_direction[0] != 0 else float('inf')\n",
    "    t_max_y = (next_voxel_boundary_y - ray_origin[1]) / ray_direction[1] if ray_direction[1] != 0 else float('inf')\n",
    "    t_max_z = (next_voxel_boundary_z - ray_origin[2]) / ray_direction[2] if ray_direction[2] != 0 else float('inf')\n",
    "    \n",
    "    t_delta_x = voxel_size / abs(ray_direction[0]) if ray_direction[0] != 0 else float('inf')\n",
    "    t_delta_y = voxel_size / abs(ray_direction[1]) if ray_direction[1] != 0 else float('inf')\n",
    "    t_delta_z = voxel_size / abs(ray_direction[2]) if ray_direction[2] != 0 else float('inf')\n",
    "\n",
    "    while True:\n",
    "        if not (0 <= ix < grid_dims[0] and 0 <= iy < grid_dims[1] and 0 <= iz < grid_dims[2]):\n",
    "            break\n",
    "        yield (ix, iy, iz)\n",
    "        if t_max_x < t_max_y:\n",
    "            if t_max_x < t_max_z:\n",
    "                ix += step_x; t_max_x += t_delta_x\n",
    "            else:\n",
    "                iz += step_z; t_max_z += t_delta_z\n",
    "        else:\n",
    "            if t_max_y < t_max_z:\n",
    "                iy += step_y; t_max_y += t_delta_y\n",
    "            else:\n",
    "                iz += step_z; t_max_z += t_delta_z\n",
    "\n",
    "def calculate_transmittance(voxel_path_generator, classification_grid, density_grid, voxel_size, k_coeffs):\n",
    "    \"\"\"\n",
    "    Calculates the final transmittance for a single ray path, using class-specific\n",
    "    extinction coefficients for different vegetation types.\n",
    "    \"\"\"\n",
    "    transmittance = 1.0\n",
    "    path_length_in_voxel = voxel_size\n",
    "    for ix, iy, iz in voxel_path_generator:\n",
    "        voxel_class = classification_grid[ix, iy, iz]\n",
    "        if voxel_class == BUILDING_CLASS_CODE: return 0.0\n",
    "        \n",
    "        # Check if the voxel is any type of vegetation\n",
    "        if voxel_class in VEGETATION_CLASS_CODES:\n",
    "            k_base = k_coeffs.get(voxel_class, 0.0) # Look up the correct k\n",
    "            if k_base > 0:\n",
    "                density = density_grid[ix, iy, iz]\n",
    "                if density > 0:\n",
    "                    k = k_base * density\n",
    "                    transmittance *= math.exp(-k * path_length_in_voxel)\n",
    "        \n",
    "        if transmittance < 1e-6: return 0.0\n",
    "    return transmittance\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Load and Prepare Data\n",
    "    points, classifications = load_and_prepare_lidar(\n",
    "        LIDAR_FILE_PATH, BOUNDING_BOX, RELEVANT_CLASSES\n",
    "    )\n",
    "    if points is None: exit()\n",
    "\n",
    "    # 2. Voxelize Scene\n",
    "    classification_grid, density_grid, scene_min, grid_dims = voxelize_scene(\n",
    "        points, classifications, VOXEL_SIZE\n",
    "    )\n",
    "    if classification_grid is None: exit()\n",
    "    scene_max = scene_min + grid_dims * VOXEL_SIZE\n",
    "\n",
    "    # 3. Define the single analysis point at the center of the scene\n",
    "    scene_center_x = (scene_min[0] + scene_max[0]) / 2\n",
    "    scene_center_y = (scene_min[1] + scene_max[1]) / 2\n",
    "\n",
    "    ground_points_all = points[classifications == GROUND_CLASS_CODE]\n",
    "    if len(ground_points_all) > 0:\n",
    "        search_radius = 5.0\n",
    "        center_points_mask = (\n",
    "            (ground_points_all[:, 0] > scene_center_x - search_radius) &\n",
    "            (ground_points_all[:, 0] < scene_center_x + search_radius) &\n",
    "            (ground_points_all[:, 1] > scene_center_y - search_radius) &\n",
    "            (ground_points_all[:, 1] < scene_center_y + search_radius)\n",
    "        )\n",
    "        ground_points_near_center = ground_points_all[center_points_mask]\n",
    "        if len(ground_points_near_center) > 0:\n",
    "            ground_z = np.min(ground_points_near_center[:, 2]) + 0.01\n",
    "        else:\n",
    "            print(\"Warning: No ground points near center. Using scene minimum Z.\")\n",
    "            ground_z = scene_min[2] + 0.01\n",
    "    else:\n",
    "        print(\"Warning: No ground points in dataset. Using scene minimum Z.\")\n",
    "        ground_z = scene_min[2] + 0.01\n",
    "\n",
    "    analysis_point = np.array([scene_center_x, scene_center_y, ground_z])\n",
    "    print(f\"Analysis point set to scene center: {analysis_point}\")\n",
    "\n",
    "    # 4. Define Solar Angles and Run Simulation Loop (SEQUENTIAL)\n",
    "    azimuths = np.linspace(0, 2 * np.pi, AZIMUTH_STEPS, endpoint=False)\n",
    "    elevations = np.linspace(0, np.pi / 2, ELEVATION_STEPS, endpoint=True)\n",
    "    solar_radius_rad = np.deg2rad(SOLAR_ANGULAR_RADIUS_DEG)\n",
    "    \n",
    "    simulation_results = []\n",
    "    \n",
    "    total_directions = len(azimuths) * (len(elevations) - 1)\n",
    "    print(f\"\\n--- Starting SEQUENTIAL Cone-Casting Simulation ---\")\n",
    "    print(f\"Casting {NUM_RAYS_PER_CONE} rays per cone for each of {total_directions} solar positions...\")\n",
    "    \n",
    "    current_direction = 0\n",
    "    for el in elevations:\n",
    "        if el < 0.001: continue\n",
    "        \n",
    "        for az in azimuths:\n",
    "            current_direction += 1\n",
    "            if current_direction % 500 == 0:\n",
    "                 print(f\"Processing direction {current_direction} of {total_directions} (Az: {np.rad2deg(az):.1f}째, El: {np.rad2deg(el):.1f}째)...\")\n",
    "            \n",
    "            center_ray_direction = np.array([np.cos(el) * np.sin(az), np.cos(el) * np.cos(az), np.sin(el)])\n",
    "            \n",
    "            cone_ray_vectors = generate_cone_vectors(center_ray_direction, solar_radius_rad, NUM_RAYS_PER_CONE)\n",
    "            \n",
    "            cone_transmittances = []\n",
    "            for ray_vec in cone_ray_vectors:\n",
    "                voxel_size = VOXEL_SIZE\n",
    "                voxel_path_gen = trace_ray_fast(analysis_point, ray_vec, scene_min, voxel_size, grid_dims)\n",
    "                transmittance = calculate_transmittance(\n",
    "                    voxel_path_gen, classification_grid, density_grid, voxel_size, VEGETATION_EXTINCTION_COEFFICIENTS\n",
    "                )\n",
    "                cone_transmittances.append(transmittance)\n",
    "            \n",
    "            avg_transmittance = np.mean(cone_transmittances)\n",
    "            \n",
    "            simulation_results.append({\n",
    "                'azimuth': az, 'elevation': el, 'transmittance': avg_transmittance\n",
    "            })\n",
    "\n",
    "    # 5. Format and Save Final Attenuation Matrix\n",
    "    print(\"\\n--- Aggregating Results into CSV Matrix ---\")\n",
    "    if not simulation_results:\n",
    "        print(\"No results to save. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    df = pd.DataFrame(simulation_results)\n",
    "    df['azimuth_deg'] = np.round(np.rad2deg(df['azimuth'])).astype(int)\n",
    "    df['elevation_deg'] = np.round(np.rad2deg(df['elevation'])).astype(int)\n",
    "    \n",
    "    shadow_matrix_df = df.pivot_table(\n",
    "        index='elevation_deg', columns='azimuth_deg', values='transmittance'\n",
    "    )\n",
    "    \n",
    "    shadow_matrix_df.index = [f\"Altitude_{i}\" for i in shadow_matrix_df.index]\n",
    "    shadow_matrix_df.columns = [f\"Azimuth_{c}\" for c in shadow_matrix_df.columns]\n",
    "    \n",
    "    shadow_matrix_df = shadow_matrix_df.sort_index(key=lambda x: np.array([int(i.split('_')[1]) for i in x]))\n",
    "    shadow_matrix_df = shadow_matrix_df.sort_index(axis=1, key=lambda x: np.array([int(i.split('_')[1]) for i in x]))\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIRECTORY):\n",
    "        os.makedirs(OUTPUT_DIRECTORY)\n",
    "        \n",
    "    output_path = os.path.join(OUTPUT_DIRECTORY, OUTPUT_FILENAME)\n",
    "    shadow_matrix_df.to_csv(output_path, header=True, index=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- Simulation Finished ---\")\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Saved shadow attenuation matrix to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxel-Based Cone-Casting Shadow Attenuation Matrix Generator\n",
    "#\n",
    "# This script implements an advanced framework to simulate light attenuation\n",
    "# using a cone-casting method to account for the sun's angular diameter,\n",
    "# producing more realistic soft shadows (penumbra).\n",
    "#\n",
    "# This version is configured to analyze a single point at the center of the LiDAR scene\n",
    "# and supports multiple vegetation classes with distinct attenuation coefficients.\n",
    "#\n",
    "# NOTE: This version has been simplified to run sequentially without Numba or\n",
    "# multiprocessing to ensure stability and ease of debugging. Performance will be\n",
    "# significantly lower than optimized versions.\n",
    "#\n",
    "# The pipeline is as follows:\n",
    "# 1. Data Ingestion: Reads and filters classified LiDAR data (.las/.laz).\n",
    "# 2. Voxelization: Converts the point cloud into a 3D voxel grid.\n",
    "# 3. Building Extrusion: Fills voxels below building roof points to create solid structures.\n",
    "# 4. Iterative Cone-Casting: For each solar position, it generates a cone of\n",
    "#    rays representing the sun's disk.\n",
    "# 5. Attenuation Modeling: For each ray in the cone, it calculates transmittance\n",
    "#    based on binary occlusion (buildings) or the Beer-Lambert law (vegetation),\n",
    "#    using class-specific extinction coefficients.\n",
    "# 6. Output Generation: The final transmittance for a solar position is the\n",
    "#    average of all rays in the cone. These values are aggregated into a\n",
    "#    2D matrix and saved as a CSV file.\n",
    "#\n",
    "# Key Libraries:\n",
    "# - laspy: For reading LiDAR files.\n",
    "# - numpy: For numerical operations and data structures.\n",
    "# - pandas: For creating and exporting the final CSV data matrix.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import laspy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Set all simulation parameters in this section.\n",
    "\n",
    "# 1. Input Data and Scene Definition\n",
    "LIDAR_FILE_PATH = 'data/houselas_re_veg_2.las'\n",
    "OUTPUT_DIRECTORY = 'results/shadow_matrix_results'\n",
    "OUTPUT_FILENAME = 'shadow_attenuation_matrix_conecasting.csv'\n",
    "BOUNDING_BOX = None\n",
    "\n",
    "# --- NEW: Specific target coordinates for analysis ---\n",
    "TARGET_COORDS_2D = np.array([532886, 6983516])\n",
    "\n",
    "# ASPRS Standard Classification Codes\n",
    "# Including multiple vegetation classes (3: Low, 4: Medium, 5: High)\n",
    "RELEVANT_CLASSES = {2, 3, 4, 5, 6}\n",
    "GROUND_CLASS_CODE = 2\n",
    "BUILDING_CLASS_CODE = 6\n",
    "VEGETATION_CLASS_CODES = {3, 4, 5}\n",
    "\n",
    "# Priority for voxel classification (higher number = higher priority)\n",
    "# High vegetation (5) has priority over medium (4), etc.\n",
    "CLASS_PRIORITY = {6: 4, 5: 3, 4: 2, 3: 1, 2: 0, 0: -1}\n",
    "\n",
    "# 2. Voxelization Parameters\n",
    "VOXEL_SIZE = 0.5\n",
    "\n",
    "# 3. Solar Position & Cone-Casting Simulation Parameters\n",
    "AZIMUTH_STEPS = 360  # 1-degree steps\n",
    "ELEVATION_STEPS = 91 # 1-degree steps (0-90 inclusive)\n",
    "\n",
    "# --- NEW: Cone-Casting Parameters ---\n",
    "# The sun's angular radius is approx 0.265 degrees.\n",
    "SOLAR_ANGULAR_RADIUS_DEG = 0.265\n",
    "# Number of rays to cast to approximate the solar disk. More rays = more\n",
    "# accurate penumbra but longer computation time.\n",
    "NUM_RAYS_PER_CONE = 16\n",
    "\n",
    "# 4. Ray-Casting and Attenuation Parameters\n",
    "# --- UPDATED: Class-specific extinction coefficients ---\n",
    "# Assign a different base extinction coefficient (k) to each vegetation class.\n",
    "# These values may need to be calibrated for specific vegetation types.\n",
    "VEGETATION_EXTINCTION_COEFFICIENTS = {\n",
    "    3: 0.7,  # k for Low Vegetation\n",
    "    4: 0.5,  # k for Medium Vegetation\n",
    "    5: 0.3   # k for High Vegetation\n",
    "}\n",
    "\n",
    "\n",
    "# --- MODULE 1: DATA INGESTOR & PREPARER ---\n",
    "\n",
    "def load_and_prepare_lidar(file_path, bounding_box=None, relevant_classes=None):\n",
    "    \"\"\"\n",
    "    Reads a LAS/LAZ file, filters points by classification and bounding box.\n",
    "    \"\"\"\n",
    "    print(f\"Loading LiDAR data from {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None, None\n",
    "    try:\n",
    "        las = laspy.read(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    points_xyz = np.vstack((las.x, las.y, las.z)).transpose()\n",
    "    classifications = np.array(las.classification)\n",
    "\n",
    "    mask = np.ones(len(points_xyz), dtype=bool)\n",
    "    if bounding_box:\n",
    "        mask &= (\n",
    "            (points_xyz[:, 0] >= bounding_box['X_MIN']) & (points_xyz[:, 0] < bounding_box['X_MAX']) &\n",
    "            (points_xyz[:, 1] >= bounding_box['Y_MIN']) & (points_xyz[:, 1] < bounding_box['Y_MAX'])\n",
    "        )\n",
    "    if relevant_classes:\n",
    "        mask &= np.isin(classifications, list(relevant_classes))\n",
    "\n",
    "    filtered_points = points_xyz[mask]\n",
    "    filtered_classifications = classifications[mask]\n",
    "\n",
    "    if len(filtered_points) == 0:\n",
    "        print(\"Warning: No points remaining after filtering.\")\n",
    "        return None, None\n",
    "    print(f\"Data loaded and filtered. {len(filtered_points)} points remaining.\")\n",
    "    return filtered_points, filtered_classifications\n",
    "\n",
    "\n",
    "# --- MODULE 2: VOXELIZER ---\n",
    "\n",
    "def voxelize_scene(points, classifications, voxel_size):\n",
    "    \"\"\"\n",
    "    Converts a point cloud into classification and density voxel grids.\n",
    "    \"\"\"\n",
    "    if points is None or len(points) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(\"Voxelizing the scene...\")\n",
    "    scene_min = np.min(points, axis=0)\n",
    "    scene_max = np.max(points, axis=0)\n",
    "    grid_dims = np.ceil((scene_max - scene_min) / voxel_size).astype(int)\n",
    "    print(f\"Voxel grid dimensions: {grid_dims}\")\n",
    "\n",
    "    classification_grid = np.zeros(grid_dims, dtype=np.int8)\n",
    "    density_grid = np.zeros(grid_dims, dtype=np.float32)\n",
    "\n",
    "    voxel_indices = np.floor((points - scene_min) / voxel_size).astype(int)\n",
    "    for i in range(3):\n",
    "        voxel_indices[:, i] = np.clip(voxel_indices[:, i], 0, grid_dims[i] - 1)\n",
    "\n",
    "    print(\"Populating classification and density grids from points...\")\n",
    "    for i in range(len(points)):\n",
    "        idx = tuple(voxel_indices[i])\n",
    "        current_class = classifications[i]\n",
    "        if CLASS_PRIORITY.get(current_class, -1) > CLASS_PRIORITY.get(classification_grid[idx], -1):\n",
    "            classification_grid[idx] = current_class\n",
    "        if current_class in VEGETATION_CLASS_CODES:\n",
    "            density_grid[idx] += 1\n",
    "\n",
    "    voxel_volume = voxel_size ** 3\n",
    "    vegetation_voxels = np.isin(classification_grid, list(VEGETATION_CLASS_CODES))\n",
    "    density_grid[vegetation_voxels] /= voxel_volume\n",
    "    \n",
    "    print(\"Voxelization complete.\")\n",
    "    return classification_grid, density_grid, scene_min, grid_dims\n",
    "\n",
    "\n",
    "# --- MODULE 3: RAY-CASTING & CONE-CASTING ENGINE ---\n",
    "\n",
    "def generate_cone_vectors(center_direction, radius_rad, num_samples):\n",
    "    \"\"\"\n",
    "    Generates a set of vectors distributed within a cone around a center direction.\n",
    "    \"\"\"\n",
    "    # Create a basis (a coordinate system) aligned with the center_direction\n",
    "    if np.allclose(np.abs(center_direction), [0, 0, 1]):\n",
    "        # Handle case where direction is along Z-axis\n",
    "        v_up = np.array([0, 1, 0])\n",
    "    else:\n",
    "        v_up = np.array([0, 0, 1])\n",
    "    \n",
    "    u = np.cross(v_up, center_direction)\n",
    "    u /= np.linalg.norm(u)\n",
    "    v = np.cross(center_direction, u)\n",
    "\n",
    "    cone_vectors = []\n",
    "    # Use stratified sampling for better distribution\n",
    "    for i in range(num_samples):\n",
    "        # Sample radius and angle to get points on a disk\n",
    "        r = radius_rad * np.sqrt((i + 0.5) / num_samples)\n",
    "        theta = 2 * np.pi * 0.61803398875 * i # Golden angle for good distribution\n",
    "\n",
    "        # Map disk point to 3D offset and add to center direction\n",
    "        offset_vec = r * (np.cos(theta) * u + np.sin(theta) * v)\n",
    "        new_vec = center_direction + offset_vec\n",
    "        new_vec /= np.linalg.norm(new_vec) # Re-normalize\n",
    "        cone_vectors.append(new_vec)\n",
    "        \n",
    "    return cone_vectors\n",
    "\n",
    "def trace_ray_fast(ray_origin, ray_direction, scene_min, voxel_size, grid_dims):\n",
    "    \"\"\"\n",
    "    An efficient voxel traversal algorithm (Amanatides-Woo).\n",
    "    \"\"\"\n",
    "    ray_pos = (ray_origin - scene_min) / voxel_size\n",
    "    ix, iy, iz = int(ray_pos[0]), int(ray_pos[1]), int(ray_pos[2])\n",
    "    step_x = 1 if ray_direction[0] >= 0 else -1\n",
    "    step_y = 1 if ray_direction[1] >= 0 else -1\n",
    "    step_z = 1 if ray_direction[2] >= 0 else -1\n",
    "    \n",
    "    next_voxel_boundary_x = (ix + (step_x > 0)) * voxel_size + scene_min[0]\n",
    "    next_voxel_boundary_y = (iy + (step_y > 0)) * voxel_size + scene_min[1]\n",
    "    next_voxel_boundary_z = (iz + (step_z > 0)) * voxel_size + scene_min[2]\n",
    "    \n",
    "    t_max_x = (next_voxel_boundary_x - ray_origin[0]) / ray_direction[0] if ray_direction[0] != 0 else float('inf')\n",
    "    t_max_y = (next_voxel_boundary_y - ray_origin[1]) / ray_direction[1] if ray_direction[1] != 0 else float('inf')\n",
    "    t_max_z = (next_voxel_boundary_z - ray_origin[2]) / ray_direction[2] if ray_direction[2] != 0 else float('inf')\n",
    "    \n",
    "    t_delta_x = voxel_size / abs(ray_direction[0]) if ray_direction[0] != 0 else float('inf')\n",
    "    t_delta_y = voxel_size / abs(ray_direction[1]) if ray_direction[1] != 0 else float('inf')\n",
    "    t_delta_z = voxel_size / abs(ray_direction[2]) if ray_direction[2] != 0 else float('inf')\n",
    "\n",
    "    while True:\n",
    "        if not (0 <= ix < grid_dims[0] and 0 <= iy < grid_dims[1] and 0 <= iz < grid_dims[2]):\n",
    "            break\n",
    "        yield (ix, iy, iz)\n",
    "        if t_max_x < t_max_y:\n",
    "            if t_max_x < t_max_z:\n",
    "                ix += step_x; t_max_x += t_delta_x\n",
    "            else:\n",
    "                iz += step_z; t_max_z += t_delta_z\n",
    "        else:\n",
    "            if t_max_y < t_max_z:\n",
    "                iy += step_y; t_max_y += t_delta_y\n",
    "            else:\n",
    "                iz += step_z; t_max_z += t_delta_z\n",
    "\n",
    "def calculate_transmittance(voxel_path_generator, classification_grid, density_grid, voxel_size, k_coeffs):\n",
    "    \"\"\"\n",
    "    Calculates the final transmittance for a single ray path, using class-specific\n",
    "    extinction coefficients for different vegetation types.\n",
    "    \"\"\"\n",
    "    transmittance = 1.0\n",
    "    path_length_in_voxel = voxel_size\n",
    "    for ix, iy, iz in voxel_path_generator:\n",
    "        voxel_class = classification_grid[ix, iy, iz]\n",
    "        if voxel_class == BUILDING_CLASS_CODE: return 0.0\n",
    "        \n",
    "        # Check if the voxel is any type of vegetation\n",
    "        if voxel_class in VEGETATION_CLASS_CODES:\n",
    "            k_base = k_coeffs.get(voxel_class, 0.0) # Look up the correct k\n",
    "            if k_base > 0:\n",
    "                density = density_grid[ix, iy, iz]\n",
    "                if density > 0:\n",
    "                    k = k_base * density\n",
    "                    transmittance *= math.exp(-k * path_length_in_voxel)\n",
    "        \n",
    "        if transmittance < 1e-6: return 0.0\n",
    "    return transmittance\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Load and Prepare Data\n",
    "    points, classifications = load_and_prepare_lidar(\n",
    "        LIDAR_FILE_PATH, BOUNDING_BOX, RELEVANT_CLASSES\n",
    "    )\n",
    "    if points is None: exit()\n",
    "\n",
    "    # 2. Voxelize Scene\n",
    "    classification_grid, density_grid, scene_min, grid_dims = voxelize_scene(\n",
    "        points, classifications, VOXEL_SIZE\n",
    "    )\n",
    "    if classification_grid is None: exit()\n",
    "    scene_max = scene_min + grid_dims * VOXEL_SIZE\n",
    "\n",
    "    # 3. Define the single analysis point using the specified coordinates\n",
    "    print(f\"Using specified target coordinates: {TARGET_COORDS_2D}\")\n",
    "\n",
    "    # Find the Z value for the highest point (e.g., a roof) at the target coordinates.\n",
    "    search_radius = 5.0 # 5 meters\n",
    "    points_near_target_mask = (\n",
    "        (points[:, 0] > TARGET_COORDS_2D[0] - search_radius) &\n",
    "        (points[:, 0] < TARGET_COORDS_2D[0] + search_radius) &\n",
    "        (points[:, 1] > TARGET_COORDS_2D[1] - search_radius) &\n",
    "        (points[:, 1] < TARGET_COORDS_2D[1] + search_radius)\n",
    "    )\n",
    "    points_near_target = points[points_near_target_mask]\n",
    "\n",
    "    if len(points_near_target) > 0:\n",
    "        # Find the highest Z value in the vicinity of the target\n",
    "        target_z = np.max(points_near_target[:, 2]) + 0.01\n",
    "    else:\n",
    "        print(f\"Warning: No LiDAR points found near target coordinates. Using scene maximum Z as a fallback.\")\n",
    "        target_z = scene_max[2]\n",
    "\n",
    "    analysis_point = np.array([TARGET_COORDS_2D[0], TARGET_COORDS_2D[1], target_z])\n",
    "    print(f\"Analysis point set to: {analysis_point}\")\n",
    "\n",
    "    # 4. Define Solar Angles and Run Simulation Loop (SEQUENTIAL)\n",
    "    azimuths = np.linspace(0, 2 * np.pi, AZIMUTH_STEPS, endpoint=False)\n",
    "    elevations = np.linspace(0, np.pi / 2, ELEVATION_STEPS, endpoint=True)\n",
    "    solar_radius_rad = np.deg2rad(SOLAR_ANGULAR_RADIUS_DEG)\n",
    "    \n",
    "    simulation_results = []\n",
    "    \n",
    "    total_directions = len(azimuths) * (len(elevations) - 1)\n",
    "    print(f\"\\n--- Starting SEQUENTIAL Cone-Casting Simulation ---\")\n",
    "    print(f\"Casting {NUM_RAYS_PER_CONE} rays per cone for each of {total_directions} solar positions...\")\n",
    "    \n",
    "    current_direction = 0\n",
    "    for el in elevations:\n",
    "        if el < 0.001: continue\n",
    "        \n",
    "        for az in azimuths:\n",
    "            current_direction += 1\n",
    "            if current_direction % 500 == 0:\n",
    "                 print(f\"Processing direction {current_direction} of {total_directions} (Az: {np.rad2deg(az):.1f}째, El: {np.rad2deg(el):.1f}째)...\")\n",
    "            \n",
    "            center_ray_direction = np.array([np.cos(el) * np.sin(az), np.cos(el) * np.cos(az), np.sin(el)])\n",
    "            \n",
    "            cone_ray_vectors = generate_cone_vectors(center_ray_direction, solar_radius_rad, NUM_RAYS_PER_CONE)\n",
    "            \n",
    "            cone_transmittances = []\n",
    "            for ray_vec in cone_ray_vectors:\n",
    "                voxel_path_gen = trace_ray_fast(analysis_point, ray_vec, scene_min, voxel_size, grid_dims)\n",
    "                transmittance = calculate_transmittance(\n",
    "                    voxel_path_gen, classification_grid, density_grid, voxel_size, VEGETATION_EXTINCTION_COEFFICIENTS\n",
    "                )\n",
    "                cone_transmittances.append(transmittance)\n",
    "            \n",
    "            avg_transmittance = np.mean(cone_transmittances)\n",
    "            \n",
    "            simulation_results.append({\n",
    "                'azimuth': az, 'elevation': el, 'transmittance': avg_transmittance\n",
    "            })\n",
    "\n",
    "    # 5. Format and Save Final Attenuation Matrix\n",
    "    print(\"\\n--- Aggregating Results into CSV Matrix ---\")\n",
    "    if not simulation_results:\n",
    "        print(\"No results to save. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    df = pd.DataFrame(simulation_results)\n",
    "    df['azimuth_deg'] = np.round(np.rad2deg(df['azimuth'])).astype(int)\n",
    "    df['elevation_deg'] = np.round(np.rad2deg(df['elevation'])).astype(int)\n",
    "    \n",
    "    shadow_matrix_df = df.pivot_table(\n",
    "        index='elevation_deg', columns='azimuth_deg', values='transmittance'\n",
    "    )\n",
    "    \n",
    "    # Sort the DataFrame by numeric index and columns to ensure correct order\n",
    "    shadow_matrix_df = shadow_matrix_df.sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIRECTORY):\n",
    "        os.makedirs(OUTPUT_DIRECTORY)\n",
    "        \n",
    "    output_path = os.path.join(OUTPUT_DIRECTORY, OUTPUT_FILENAME)\n",
    "    # Write to CSV without header or index\n",
    "    shadow_matrix_df.to_csv(output_path, header=False, index=False)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- Simulation Finished ---\")\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Saved shadow attenuation matrix to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxel-Based Cone-Casting Shadow Attenuation Matrix Generator\n",
    "#\n",
    "# This script implements an advanced framework to simulate light attenuation\n",
    "# using a cone-casting method to account for the sun's angular diameter,\n",
    "# producing more realistic soft shadows (penumbra).\n",
    "#\n",
    "# This version is configured to analyze a single point at the center of the LiDAR scene\n",
    "# and supports multiple vegetation classes with distinct attenuation coefficients.\n",
    "#\n",
    "# NOTE: This version has been simplified to run sequentially without Numba or\n",
    "# multiprocessing to ensure stability and ease of debugging. Performance will be\n",
    "# significantly lower than optimized versions.\n",
    "#\n",
    "# The pipeline is as follows:\n",
    "# 1. Data Ingestion: Reads and filters classified LiDAR data (.las/.laz).\n",
    "# 2. Voxelization: Converts the point cloud into a 3D voxel grid.\n",
    "# 3. Building Extrusion: Fills voxels below building roof points to create solid structures.\n",
    "# 4. Iterative Cone-Casting: For each solar position, it generates a cone of\n",
    "#    rays representing the sun's disk.\n",
    "# 5. Attenuation Modeling: For each ray in the cone, it calculates transmittance\n",
    "#    based on binary occlusion (buildings) or the Beer-Lambert law (vegetation),\n",
    "#    using class-specific extinction coefficients.\n",
    "# 6. Output Generation: The final transmittance for a solar position is the\n",
    "#    average of all rays in the cone. These values are aggregated into a\n",
    "#    2D matrix and saved as a CSV file.\n",
    "#\n",
    "# Key Libraries:\n",
    "# - laspy: For reading LiDAR files.\n",
    "# - numpy: For numerical operations and data structures.\n",
    "# - pandas: For creating and exporting the final CSV data matrix.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import laspy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Set all simulation parameters in this section.\n",
    "\n",
    "# 1. Input Data and Scene Definition\n",
    "LIDAR_FILE_PATH = 'data/houselas_re_veg_2.las'\n",
    "OUTPUT_DIRECTORY = 'results/shadow_matrix_results'\n",
    "OUTPUT_FILENAME = 'shadow_attenuation_matrix_conecasting.csv'\n",
    "BOUNDING_BOX = None\n",
    "\n",
    "# --- NEW: Specific target coordinates for analysis ---\n",
    "TARGET_COORDS_2D = np.array([532886, 6983516])\n",
    "\n",
    "# ASPRS Standard Classification Codes\n",
    "# Including multiple vegetation classes (3: Low, 4: Medium, 5: High)\n",
    "RELEVANT_CLASSES = {2, 3, 4, 5, 6}\n",
    "GROUND_CLASS_CODE = 2\n",
    "BUILDING_CLASS_CODE = 6\n",
    "VEGETATION_CLASS_CODES = {3, 4, 5}\n",
    "\n",
    "# Priority for voxel classification (higher number = higher priority)\n",
    "# High vegetation (5) has priority over medium (4), etc.\n",
    "CLASS_PRIORITY = {6: 4, 5: 3, 4: 2, 3: 1, 2: 0, 0: -1}\n",
    "\n",
    "# 2. Voxelization Parameters\n",
    "VOXEL_SIZE = 0.5\n",
    "\n",
    "# 3. Solar Position & Cone-Casting Simulation Parameters\n",
    "AZIMUTH_STEPS = 360  # 1-degree steps\n",
    "ELEVATION_STEPS = 91 # 1-degree steps (0-90 inclusive)\n",
    "\n",
    "# --- NEW: Cone-Casting Parameters ---\n",
    "# The sun's angular radius is approx 0.265 degrees.\n",
    "SOLAR_ANGULAR_RADIUS_DEG = 0.265\n",
    "# Number of rays to cast to approximate the solar disk. More rays = more\n",
    "# accurate penumbra but longer computation time.\n",
    "NUM_RAYS_PER_CONE = 16\n",
    "\n",
    "# 4. Ray-Casting and Attenuation Parameters\n",
    "# --- UPDATED: Class-specific extinction coefficients ---\n",
    "# Assign a different base extinction coefficient (k) to each vegetation class.\n",
    "# These values may need to be calibrated for specific vegetation types.\n",
    "VEGETATION_EXTINCTION_COEFFICIENTS = {\n",
    "    3: 0.7,  # k for Low Vegetation\n",
    "    4: 0.5,  # k for Medium Vegetation\n",
    "    5: 0.3   # k for High Vegetation\n",
    "}\n",
    "\n",
    "\n",
    "# --- MODULE 1: DATA INGESTOR & PREPARER ---\n",
    "\n",
    "def load_and_prepare_lidar(file_path, bounding_box=None, relevant_classes=None):\n",
    "    \"\"\"\n",
    "    Reads a LAS/LAZ file, filters points by classification and bounding box.\n",
    "    \"\"\"\n",
    "    print(f\"Loading LiDAR data from {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None, None\n",
    "    try:\n",
    "        las = laspy.read(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    points_xyz = np.vstack((las.x, las.y, las.z)).transpose()\n",
    "    classifications = np.array(las.classification)\n",
    "\n",
    "    mask = np.ones(len(points_xyz), dtype=bool)\n",
    "    if bounding_box:\n",
    "        mask &= (\n",
    "            (points_xyz[:, 0] >= bounding_box['X_MIN']) & (points_xyz[:, 0] < bounding_box['X_MAX']) &\n",
    "            (points_xyz[:, 1] >= bounding_box['Y_MIN']) & (points_xyz[:, 1] < bounding_box['Y_MAX'])\n",
    "        )\n",
    "    if relevant_classes:\n",
    "        mask &= np.isin(classifications, list(relevant_classes))\n",
    "\n",
    "    filtered_points = points_xyz[mask]\n",
    "    filtered_classifications = classifications[mask]\n",
    "\n",
    "    if len(filtered_points) == 0:\n",
    "        print(\"Warning: No points remaining after filtering.\")\n",
    "        return None, None\n",
    "    print(f\"Data loaded and filtered. {len(filtered_points)} points remaining.\")\n",
    "    return filtered_points, filtered_classifications\n",
    "\n",
    "\n",
    "# --- MODULE 2: VOXELIZER ---\n",
    "\n",
    "def voxelize_scene(points, classifications, voxel_size):\n",
    "    \"\"\"\n",
    "    Converts a point cloud into classification and density voxel grids.\n",
    "    Includes a step to extrude building footprints downwards.\n",
    "    \"\"\"\n",
    "    if points is None or len(points) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(\"Voxelizing the scene...\")\n",
    "    scene_min = np.min(points, axis=0)\n",
    "    scene_max = np.max(points, axis=0)\n",
    "    grid_dims = np.ceil((scene_max - scene_min) / voxel_size).astype(int)\n",
    "    print(f\"Voxel grid dimensions: {grid_dims}\")\n",
    "\n",
    "    classification_grid = np.zeros(grid_dims, dtype=np.int8)\n",
    "    density_grid = np.zeros(grid_dims, dtype=np.float32)\n",
    "\n",
    "    voxel_indices = np.floor((points - scene_min) / voxel_size).astype(int)\n",
    "    for i in range(3):\n",
    "        voxel_indices[:, i] = np.clip(voxel_indices[:, i], 0, grid_dims[i] - 1)\n",
    "\n",
    "    print(\"Populating classification and density grids from points...\")\n",
    "    for i in range(len(points)):\n",
    "        idx = tuple(voxel_indices[i])\n",
    "        current_class = classifications[i]\n",
    "        if CLASS_PRIORITY.get(current_class, -1) > CLASS_PRIORITY.get(classification_grid[idx], -1):\n",
    "            classification_grid[idx] = current_class\n",
    "        if current_class in VEGETATION_CLASS_CODES:\n",
    "            density_grid[idx] += 1\n",
    "\n",
    "    # --- NEW: Extrude building footprints downwards ---\n",
    "    print(\"Extruding building footprints to create solid models...\")\n",
    "    building_indices = np.argwhere(classification_grid == BUILDING_CLASS_CODE)\n",
    "\n",
    "    for ix, iy, iz in building_indices:\n",
    "        # Iterate from the voxel below the roof down to the ground level (z=0)\n",
    "        for z_level in range(iz - 1, -1, -1):\n",
    "            # Only fill if the voxel below has a lower priority class\n",
    "            if CLASS_PRIORITY.get(classification_grid[ix, iy, z_level], -1) < CLASS_PRIORITY[BUILDING_CLASS_CODE]:\n",
    "                classification_grid[ix, iy, z_level] = BUILDING_CLASS_CODE\n",
    "            else:\n",
    "                # Stop if we hit something with equal or higher priority (e.g., another building part)\n",
    "                break\n",
    "    print(\"Building extrusion complete.\")\n",
    "\n",
    "    voxel_volume = voxel_size ** 3\n",
    "    vegetation_voxels = np.isin(classification_grid, list(VEGETATION_CLASS_CODES))\n",
    "    density_grid[vegetation_voxels] /= voxel_volume\n",
    "    \n",
    "    print(\"Voxelization complete.\")\n",
    "    return classification_grid, density_grid, scene_min, grid_dims\n",
    "\n",
    "\n",
    "# --- MODULE 3: RAY-CASTING & CONE-CASTING ENGINE ---\n",
    "\n",
    "def generate_cone_vectors(center_direction, radius_rad, num_samples):\n",
    "    \"\"\"\n",
    "    Generates a set of vectors distributed within a cone around a center direction.\n",
    "    \"\"\"\n",
    "    # Create a basis (a coordinate system) aligned with the center_direction\n",
    "    if np.allclose(np.abs(center_direction), [0, 0, 1]):\n",
    "        # Handle case where direction is along Z-axis\n",
    "        v_up = np.array([0, 1, 0])\n",
    "    else:\n",
    "        v_up = np.array([0, 0, 1])\n",
    "    \n",
    "    u = np.cross(v_up, center_direction)\n",
    "    u /= np.linalg.norm(u)\n",
    "    v = np.cross(center_direction, u)\n",
    "\n",
    "    cone_vectors = []\n",
    "    # Use stratified sampling for better distribution\n",
    "    for i in range(num_samples):\n",
    "        # Sample radius and angle to get points on a disk\n",
    "        r = radius_rad * np.sqrt((i + 0.5) / num_samples)\n",
    "        theta = 2 * np.pi * 0.61803398875 * i # Golden angle for good distribution\n",
    "\n",
    "        # Map disk point to 3D offset and add to center direction\n",
    "        offset_vec = r * (np.cos(theta) * u + np.sin(theta) * v)\n",
    "        new_vec = center_direction + offset_vec\n",
    "        new_vec /= np.linalg.norm(new_vec) # Re-normalize\n",
    "        cone_vectors.append(new_vec)\n",
    "        \n",
    "    return cone_vectors\n",
    "\n",
    "def trace_ray_fast(ray_origin, ray_direction, scene_min, voxel_size, grid_dims):\n",
    "    \"\"\"\n",
    "    An efficient voxel traversal algorithm (Amanatides-Woo).\n",
    "    \"\"\"\n",
    "    ray_pos = (ray_origin - scene_min) / voxel_size\n",
    "    ix, iy, iz = int(ray_pos[0]), int(ray_pos[1]), int(ray_pos[2])\n",
    "    step_x = 1 if ray_direction[0] >= 0 else -1\n",
    "    step_y = 1 if ray_direction[1] >= 0 else -1\n",
    "    step_z = 1 if ray_direction[2] >= 0 else -1\n",
    "    \n",
    "    next_voxel_boundary_x = (ix + (step_x > 0)) * voxel_size + scene_min[0]\n",
    "    next_voxel_boundary_y = (iy + (step_y > 0)) * voxel_size + scene_min[1]\n",
    "    next_voxel_boundary_z = (iz + (step_z > 0)) * voxel_size + scene_min[2]\n",
    "    \n",
    "    t_max_x = (next_voxel_boundary_x - ray_origin[0]) / ray_direction[0] if ray_direction[0] != 0 else float('inf')\n",
    "    t_max_y = (next_voxel_boundary_y - ray_origin[1]) / ray_direction[1] if ray_direction[1] != 0 else float('inf')\n",
    "    t_max_z = (next_voxel_boundary_z - ray_origin[2]) / ray_direction[2] if ray_direction[2] != 0 else float('inf')\n",
    "    \n",
    "    t_delta_x = voxel_size / abs(ray_direction[0]) if ray_direction[0] != 0 else float('inf')\n",
    "    t_delta_y = voxel_size / abs(ray_direction[1]) if ray_direction[1] != 0 else float('inf')\n",
    "    t_delta_z = voxel_size / abs(ray_direction[2]) if ray_direction[2] != 0 else float('inf')\n",
    "\n",
    "    while True:\n",
    "        if not (0 <= ix < grid_dims[0] and 0 <= iy < grid_dims[1] and 0 <= iz < grid_dims[2]):\n",
    "            break\n",
    "        yield (ix, iy, iz)\n",
    "        if t_max_x < t_max_y:\n",
    "            if t_max_x < t_max_z:\n",
    "                ix += step_x; t_max_x += t_delta_x\n",
    "            else:\n",
    "                iz += step_z; t_max_z += t_delta_z\n",
    "        else:\n",
    "            if t_max_y < t_max_z:\n",
    "                iy += step_y; t_max_y += t_delta_y\n",
    "            else:\n",
    "                iz += step_z; t_max_z += t_delta_z\n",
    "\n",
    "def calculate_transmittance(voxel_path_generator, classification_grid, density_grid, voxel_size, k_coeffs):\n",
    "    \"\"\"\n",
    "    Calculates the final transmittance for a single ray path, using class-specific\n",
    "    extinction coefficients for different vegetation types.\n",
    "    \"\"\"\n",
    "    transmittance = 1.0\n",
    "    path_length_in_voxel = voxel_size\n",
    "    for ix, iy, iz in voxel_path_generator:\n",
    "        voxel_class = classification_grid[ix, iy, iz]\n",
    "        if voxel_class == BUILDING_CLASS_CODE: return 0.0\n",
    "        \n",
    "        # Check if the voxel is any type of vegetation\n",
    "        if voxel_class in VEGETATION_CLASS_CODES:\n",
    "            k_base = k_coeffs.get(voxel_class, 0.0) # Look up the correct k\n",
    "            if k_base > 0:\n",
    "                density = density_grid[ix, iy, iz]\n",
    "                if density > 0:\n",
    "                    k = k_base * density\n",
    "                    transmittance *= math.exp(-k * path_length_in_voxel)\n",
    "        \n",
    "        if transmittance < 1e-6: return 0.0\n",
    "    return transmittance\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Load and Prepare Data\n",
    "    points, classifications = load_and_prepare_lidar(\n",
    "        LIDAR_FILE_PATH, BOUNDING_BOX, RELEVANT_CLASSES\n",
    "    )\n",
    "    if points is None: exit()\n",
    "\n",
    "    # 2. Voxelize Scene\n",
    "    classification_grid, density_grid, scene_min, grid_dims = voxelize_scene(\n",
    "        points, classifications, VOXEL_SIZE\n",
    "    )\n",
    "    if classification_grid is None: exit()\n",
    "    scene_max = scene_min + grid_dims * VOXEL_SIZE\n",
    "\n",
    "    # 3. Define the single analysis point using the specified coordinates\n",
    "    print(f\"Using specified target coordinates: {TARGET_COORDS_2D}\")\n",
    "\n",
    "    # Find the Z value for the highest point (e.g., a roof) at the target coordinates.\n",
    "    search_radius = 5.0 # 5 meters\n",
    "    points_near_target_mask = (\n",
    "        (points[:, 0] > TARGET_COORDS_2D[0] - search_radius) &\n",
    "        (points[:, 0] < TARGET_COORDS_2D[0] + search_radius) &\n",
    "        (points[:, 1] > TARGET_COORDS_2D[1] - search_radius) &\n",
    "        (points[:, 1] < TARGET_COORDS_2D[1] + search_radius)\n",
    "    )\n",
    "    points_near_target = points[points_near_target_mask]\n",
    "\n",
    "    if len(points_near_target) > 0:\n",
    "        # Find the highest Z value in the vicinity of the target\n",
    "        target_z = np.max(points_near_target[:, 2]) + 0.01\n",
    "    else:\n",
    "        print(f\"Warning: No LiDAR points found near target coordinates. Using scene maximum Z as a fallback.\")\n",
    "        target_z = scene_max[2]\n",
    "\n",
    "    analysis_point = np.array([TARGET_COORDS_2D[0], TARGET_COORDS_2D[1], target_z])\n",
    "    print(f\"Analysis point set to: {analysis_point}\")\n",
    "\n",
    "    # 4. Define Solar Angles and Run Simulation Loop (SEQUENTIAL)\n",
    "    azimuths = np.linspace(0, 2 * np.pi, AZIMUTH_STEPS, endpoint=False)\n",
    "    elevations = np.linspace(0, np.pi / 2, ELEVATION_STEPS, endpoint=True)\n",
    "    solar_radius_rad = np.deg2rad(SOLAR_ANGULAR_RADIUS_DEG)\n",
    "    \n",
    "    simulation_results = []\n",
    "    \n",
    "    total_directions = len(azimuths) * (len(elevations) - 1)\n",
    "    print(f\"\\n--- Starting SEQUENTIAL Cone-Casting Simulation ---\")\n",
    "    print(f\"Casting {NUM_RAYS_PER_CONE} rays per cone for each of {total_directions} solar positions...\")\n",
    "    \n",
    "    current_direction = 0\n",
    "    for el in elevations:\n",
    "        if el < 0.001: continue\n",
    "        \n",
    "        for az in azimuths:\n",
    "            current_direction += 1\n",
    "            if current_direction % 500 == 0:\n",
    "                 print(f\"Processing direction {current_direction} of {total_directions} (Az: {np.rad2deg(az):.1f}째, El: {np.rad2deg(el):.1f}째)...\")\n",
    "            \n",
    "            center_ray_direction = np.array([np.cos(el) * np.sin(az), np.cos(el) * np.cos(az), np.sin(el)])\n",
    "            \n",
    "            cone_ray_vectors = generate_cone_vectors(center_ray_direction, solar_radius_rad, NUM_RAYS_PER_CONE)\n",
    "            \n",
    "            cone_transmittances = []\n",
    "            for ray_vec in cone_ray_vectors:\n",
    "                voxel_path_gen = trace_ray_fast(analysis_point, ray_vec, scene_min, voxel_size, grid_dims)\n",
    "                transmittance = calculate_transmittance(\n",
    "                    voxel_path_gen, classification_grid, density_grid, voxel_size, VEGETATION_EXTINCTION_COEFFICIENTS\n",
    "                )\n",
    "                cone_transmittances.append(transmittance)\n",
    "            \n",
    "            avg_transmittance = np.mean(cone_transmittances)\n",
    "            \n",
    "            simulation_results.append({\n",
    "                'azimuth': az, 'elevation': el, 'transmittance': avg_transmittance\n",
    "            })\n",
    "\n",
    "    # 5. Format and Save Final Attenuation Matrix\n",
    "    print(\"\\n--- Aggregating Results into CSV Matrix ---\")\n",
    "    if not simulation_results:\n",
    "        print(\"No results to save. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    df = pd.DataFrame(simulation_results)\n",
    "    df['azimuth_deg'] = np.round(np.rad2deg(df['azimuth'])).astype(int)\n",
    "    df['elevation_deg'] = np.round(np.rad2deg(df['elevation'])).astype(int)\n",
    "    \n",
    "    shadow_matrix_df = df.pivot_table(\n",
    "        index='elevation_deg', columns='azimuth_deg', values='transmittance'\n",
    "    )\n",
    "    \n",
    "    # Sort the DataFrame by numeric index and columns to ensure correct order\n",
    "    shadow_matrix_df = shadow_matrix_df.sort_index(axis=0).sort_index(axis=1)\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIRECTORY):\n",
    "        os.makedirs(OUTPUT_DIRECTORY)\n",
    "        \n",
    "    output_path = os.path.join(OUTPUT_DIRECTORY, OUTPUT_FILENAME)\n",
    "    # Write to CSV without header or index\n",
    "    shadow_matrix_df.to_csv(output_path, header=True, index=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- Simulation Finished ---\")\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Saved shadow attenuation matrix to {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voxel-Based Cone-Casting Shadow Attenuation Matrix Generator\n",
    "#\n",
    "# This script implements an advanced framework to simulate light attenuation\n",
    "# using a cone-casting method to account for the sun's angular diameter,\n",
    "# producing more realistic soft shadows (penumbra).\n",
    "#\n",
    "# This version is configured to analyze a single point at the center of the LiDAR scene\n",
    "# and supports multiple vegetation classes with distinct attenuation coefficients.\n",
    "#\n",
    "# NOTE: This version has been simplified to run sequentially without Numba or\n",
    "# multiprocessing to ensure stability and ease of debugging. Performance will be\n",
    "# significantly lower than optimized versions.\n",
    "#\n",
    "# The pipeline is as follows:\n",
    "# 1. Data Ingestion: Reads and filters classified LiDAR data (.las/.laz).\n",
    "# 2. Voxelization: Converts the point cloud into a 3D voxel grid.\n",
    "# 3. Building Extrusion: Fills voxels below building roof points to create solid structures.\n",
    "# 4. Iterative Cone-Casting: For each solar position, it generates a cone of\n",
    "#    rays representing the sun's disk.\n",
    "# 5. Attenuation Modeling: For each ray in the cone, it calculates transmittance\n",
    "#    based on binary occlusion (buildings) or the Beer-Lambert law (vegetation),\n",
    "#    using class-specific extinction coefficients.\n",
    "# 6. Output Generation: The final transmittance for a solar position is the\n",
    "#    average of all rays in the cone. These values are aggregated into a\n",
    "#    2D matrix and saved as a CSV file.\n",
    "#\n",
    "# Key Libraries:\n",
    "# - laspy: For reading LiDAR files.\n",
    "# - numpy: For numerical operations and data structures.\n",
    "# - pandas: For creating and exporting the final CSV data matrix.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "\n",
    "import laspy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# Set all simulation parameters in this section.\n",
    "\n",
    "# 1. Input Data and Scene Definition\n",
    "LIDAR_FILE_PATH = 'data/houselas_re_veg_2.las'\n",
    "OUTPUT_DIRECTORY = 'results/shadow_matrix_results'\n",
    "OUTPUT_FILENAME = 'shadow_attenuation_matrix_conecasting.csv'\n",
    "BOUNDING_BOX = None\n",
    "\n",
    "\n",
    "# --- NEW: Specific target coordinates for analysis ---\n",
    "TARGET_COORDS_2D = np.array([532886, 6983516])\n",
    "\n",
    "# ASPRS Standard Classification Codes\n",
    "# Including multiple vegetation classes (3: Low, 4: Medium, 5: High)\n",
    "RELEVANT_CLASSES = {2, 3, 4, 5, 6}\n",
    "GROUND_CLASS_CODE = 2\n",
    "BUILDING_CLASS_CODE = 6\n",
    "VEGETATION_CLASS_CODES = {3, 4, 5}\n",
    "\n",
    "# Priority for voxel classification (higher number = higher priority)\n",
    "# High vegetation (5) has priority over medium (4), etc.\n",
    "CLASS_PRIORITY = {6: 4, 5: 3, 4: 2, 3: 1, 2: 0, 0: -1}\n",
    "\n",
    "# 2. Voxelization Parameters\n",
    "VOXEL_SIZE = 0.5\n",
    "\n",
    "# 3. Solar Position & Cone-Casting Simulation Parameters\n",
    "AZIMUTH_STEPS = 360  # 1-degree steps\n",
    "ELEVATION_STEPS = 91 # 1-degree steps (0-90 inclusive)\n",
    "\n",
    "# --- NEW: Cone-Casting Parameters ---\n",
    "# The sun's angular radius is approx 0.265 degrees.\n",
    "SOLAR_ANGULAR_RADIUS_DEG = 0.265\n",
    "# Number of rays to cast to approximate the solar disk. More rays = more\n",
    "# accurate penumbra but longer computation time.\n",
    "NUM_RAYS_PER_CONE = 16\n",
    "\n",
    "# 4. Ray-Casting and Attenuation Parameters\n",
    "# --- UPDATED: Class-specific extinction coefficients ---\n",
    "# Assign a different base extinction coefficient (k) to each vegetation class.\n",
    "# These values may need to be calibrated for specific vegetation types.\n",
    "VEGETATION_EXTINCTION_COEFFICIENTS = {\n",
    "    3: 0.7,  # k for Low Vegetation\n",
    "    4: 0.5,  # k for Medium Vegetation\n",
    "    5: 0.3   # k for High Vegetation\n",
    "}\n",
    "\n",
    "\n",
    "# --- MODULE 1: DATA INGESTOR & PREPARER ---\n",
    "\n",
    "def load_and_prepare_lidar(file_path, bounding_box=None, relevant_classes=None):\n",
    "    \"\"\"\n",
    "    Reads a LAS/LAZ file, filters points by classification and bounding box.\n",
    "    \"\"\"\n",
    "    print(f\"Loading LiDAR data from {file_path}...\")\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return None, None\n",
    "    try:\n",
    "        las = laspy.read(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return None, None\n",
    "\n",
    "    points_xyz = np.vstack((las.x, las.y, las.z)).transpose()\n",
    "    classifications = np.array(las.classification)\n",
    "\n",
    "    mask = np.ones(len(points_xyz), dtype=bool)\n",
    "    if bounding_box:\n",
    "        mask &= (\n",
    "            (points_xyz[:, 0] >= bounding_box['X_MIN']) & (points_xyz[:, 0] < bounding_box['X_MAX']) &\n",
    "            (points_xyz[:, 1] >= bounding_box['Y_MIN']) & (points_xyz[:, 1] < bounding_box['Y_MAX'])\n",
    "        )\n",
    "    if relevant_classes:\n",
    "        mask &= np.isin(classifications, list(relevant_classes))\n",
    "\n",
    "    filtered_points = points_xyz[mask]\n",
    "    filtered_classifications = classifications[mask]\n",
    "\n",
    "    if len(filtered_points) == 0:\n",
    "        print(\"Warning: No points remaining after filtering.\")\n",
    "        return None, None\n",
    "    print(f\"Data loaded and filtered. {len(filtered_points)} points remaining.\")\n",
    "    return filtered_points, filtered_classifications\n",
    "\n",
    "\n",
    "# --- MODULE 2: VOXELIZER ---\n",
    "\n",
    "def voxelize_scene(points, classifications, voxel_size):\n",
    "    \"\"\"\n",
    "    Converts a point cloud into classification and density voxel grids.\n",
    "    Includes a step to extrude building footprints downwards.\n",
    "    \"\"\"\n",
    "    if points is None or len(points) == 0:\n",
    "        return None, None, None, None\n",
    "\n",
    "    print(\"Voxelizing the scene...\")\n",
    "    scene_min = np.min(points, axis=0)\n",
    "    scene_max = np.max(points, axis=0)\n",
    "    grid_dims = np.ceil((scene_max - scene_min) / voxel_size).astype(int)\n",
    "    print(f\"Voxel grid dimensions: {grid_dims}\")\n",
    "\n",
    "    classification_grid = np.zeros(grid_dims, dtype=np.int8)\n",
    "    density_grid = np.zeros(grid_dims, dtype=np.float32)\n",
    "\n",
    "    voxel_indices = np.floor((points - scene_min) / voxel_size).astype(int)\n",
    "    for i in range(3):\n",
    "        voxel_indices[:, i] = np.clip(voxel_indices[:, i], 0, grid_dims[i] - 1)\n",
    "\n",
    "    print(\"Populating classification and density grids from points...\")\n",
    "    for i in range(len(points)):\n",
    "        idx = tuple(voxel_indices[i])\n",
    "        current_class = classifications[i]\n",
    "        if CLASS_PRIORITY.get(current_class, -1) > CLASS_PRIORITY.get(classification_grid[idx], -1):\n",
    "            classification_grid[idx] = current_class\n",
    "        if current_class in VEGETATION_CLASS_CODES:\n",
    "            density_grid[idx] += 1\n",
    "\n",
    "    # --- NEW: Extrude building footprints downwards ---\n",
    "    print(\"Extruding building footprints to create solid models...\")\n",
    "    building_indices = np.argwhere(classification_grid == BUILDING_CLASS_CODE)\n",
    "\n",
    "    for ix, iy, iz in building_indices:\n",
    "        # Iterate from the voxel below the roof down to the ground level (z=0)\n",
    "        for z_level in range(iz - 1, -1, -1):\n",
    "            # Only fill if the voxel below has a lower priority class\n",
    "            if CLASS_PRIORITY.get(classification_grid[ix, iy, z_level], -1) < CLASS_PRIORITY[BUILDING_CLASS_CODE]:\n",
    "                classification_grid[ix, iy, z_level] = BUILDING_CLASS_CODE\n",
    "            else:\n",
    "                # Stop if we hit something with equal or higher priority (e.g., another building part)\n",
    "                break\n",
    "    print(\"Building extrusion complete.\")\n",
    "\n",
    "    voxel_volume = voxel_size ** 3\n",
    "    vegetation_voxels = np.isin(classification_grid, list(VEGETATION_CLASS_CODES))\n",
    "    density_grid[vegetation_voxels] /= voxel_volume\n",
    "    \n",
    "    print(\"Voxelization complete.\")\n",
    "    return classification_grid, density_grid, scene_min, grid_dims\n",
    "\n",
    "\n",
    "# --- MODULE 3: RAY-CASTING & CONE-CASTING ENGINE ---\n",
    "\n",
    "def generate_cone_vectors(center_direction, radius_rad, num_samples):\n",
    "    \"\"\"\n",
    "    Generates a set of vectors distributed within a cone around a center direction.\n",
    "    \"\"\"\n",
    "    # Create a basis (a coordinate system) aligned with the center_direction\n",
    "    if np.allclose(np.abs(center_direction), [0, 0, 1]):\n",
    "        # Handle case where direction is along Z-axis\n",
    "        v_up = np.array([0, 1, 0])\n",
    "    else:\n",
    "        v_up = np.array([0, 0, 1])\n",
    "    \n",
    "    u = np.cross(v_up, center_direction)\n",
    "    u /= np.linalg.norm(u)\n",
    "    v = np.cross(center_direction, u)\n",
    "\n",
    "    cone_vectors = []\n",
    "    # Use stratified sampling for better distribution\n",
    "    for i in range(num_samples):\n",
    "        # Sample radius and angle to get points on a disk\n",
    "        r = radius_rad * np.sqrt((i + 0.5) / num_samples)\n",
    "        theta = 2 * np.pi * 0.61803398875 * i # Golden angle for good distribution\n",
    "\n",
    "        # Map disk point to 3D offset and add to center direction\n",
    "        offset_vec = r * (np.cos(theta) * u + np.sin(theta) * v)\n",
    "        new_vec = center_direction + offset_vec\n",
    "        new_vec /= np.linalg.norm(new_vec) # Re-normalize\n",
    "        cone_vectors.append(new_vec)\n",
    "        \n",
    "    return cone_vectors\n",
    "\n",
    "def trace_ray_fast(ray_origin, ray_direction, scene_min, voxel_size, grid_dims):\n",
    "    \"\"\"\n",
    "    An efficient voxel traversal algorithm (Amanatides-Woo).\n",
    "    \"\"\"\n",
    "    ray_pos = (ray_origin - scene_min) / voxel_size\n",
    "    ix, iy, iz = int(ray_pos[0]), int(ray_pos[1]), int(ray_pos[2])\n",
    "    step_x = 1 if ray_direction[0] >= 0 else -1\n",
    "    step_y = 1 if ray_direction[1] >= 0 else -1\n",
    "    step_z = 1 if ray_direction[2] >= 0 else -1\n",
    "    \n",
    "    next_voxel_boundary_x = (ix + (step_x > 0)) * voxel_size + scene_min[0]\n",
    "    next_voxel_boundary_y = (iy + (step_y > 0)) * voxel_size + scene_min[1]\n",
    "    next_voxel_boundary_z = (iz + (step_z > 0)) * voxel_size + scene_min[2]\n",
    "    \n",
    "    t_max_x = (next_voxel_boundary_x - ray_origin[0]) / ray_direction[0] if ray_direction[0] != 0 else float('inf')\n",
    "    t_max_y = (next_voxel_boundary_y - ray_origin[1]) / ray_direction[1] if ray_direction[1] != 0 else float('inf')\n",
    "    t_max_z = (next_voxel_boundary_z - ray_origin[2]) / ray_direction[2] if ray_direction[2] != 0 else float('inf')\n",
    "    \n",
    "    t_delta_x = voxel_size / abs(ray_direction[0]) if ray_direction[0] != 0 else float('inf')\n",
    "    t_delta_y = voxel_size / abs(ray_direction[1]) if ray_direction[1] != 0 else float('inf')\n",
    "    t_delta_z = voxel_size / abs(ray_direction[2]) if ray_direction[2] != 0 else float('inf')\n",
    "\n",
    "    while True:\n",
    "        if not (0 <= ix < grid_dims[0] and 0 <= iy < grid_dims[1] and 0 <= iz < grid_dims[2]):\n",
    "            break\n",
    "        yield (ix, iy, iz)\n",
    "        if t_max_x < t_max_y:\n",
    "            if t_max_x < t_max_z:\n",
    "                ix += step_x; t_max_x += t_delta_x\n",
    "            else:\n",
    "                iz += step_z; t_max_z += t_delta_z\n",
    "        else:\n",
    "            if t_max_y < t_max_z:\n",
    "                iy += step_y; t_max_y += t_delta_y\n",
    "            else:\n",
    "                iz += step_z; t_max_z += t_delta_z\n",
    "\n",
    "def calculate_transmittance(voxel_path_generator, classification_grid, density_grid, voxel_size, k_coeffs):\n",
    "    \"\"\"\n",
    "    Calculates the final transmittance for a single ray path, using class-specific\n",
    "    extinction coefficients for different vegetation types.\n",
    "    \"\"\"\n",
    "    transmittance = 1.0\n",
    "    path_length_in_voxel = voxel_size\n",
    "    for ix, iy, iz in voxel_path_generator:\n",
    "        voxel_class = classification_grid[ix, iy, iz]\n",
    "        if voxel_class == BUILDING_CLASS_CODE: return 0.0\n",
    "        \n",
    "        # Check if the voxel is any type of vegetation\n",
    "        if voxel_class in VEGETATION_CLASS_CODES:\n",
    "            k_base = k_coeffs.get(voxel_class, 0.0) # Look up the correct k\n",
    "            if k_base > 0:\n",
    "                density = density_grid[ix, iy, iz]\n",
    "                if density > 0:\n",
    "                    k = k_base * density\n",
    "                    transmittance *= math.exp(-k * path_length_in_voxel)\n",
    "        \n",
    "        if transmittance < 1e-6: return 0.0\n",
    "    return transmittance\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION ---\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 1. Load and Prepare Data\n",
    "    points, classifications = load_and_prepare_lidar(\n",
    "        LIDAR_FILE_PATH, BOUNDING_BOX, RELEVANT_CLASSES\n",
    "    )\n",
    "    if points is None: exit()\n",
    "\n",
    "    # 2. Voxelize Scene\n",
    "    classification_grid, density_grid, scene_min, grid_dims = voxelize_scene(\n",
    "        points, classifications, VOXEL_SIZE\n",
    "    )\n",
    "    if classification_grid is None: exit()\n",
    "    scene_max = scene_min + grid_dims * VOXEL_SIZE\n",
    "\n",
    "    # 3. Define the single analysis point using the specified coordinates\n",
    "    print(f\"Using specified target coordinates: {TARGET_COORDS_2D}\")\n",
    "\n",
    "    # Find the Z value for the highest point (e.g., a roof) at the target coordinates.\n",
    "    search_radius = 5.0 # 5 meters\n",
    "    points_near_target_mask = (\n",
    "        (points[:, 0] > TARGET_COORDS_2D[0] - search_radius) &\n",
    "        (points[:, 0] < TARGET_COORDS_2D[0] + search_radius) &\n",
    "        (points[:, 1] > TARGET_COORDS_2D[1] - search_radius) &\n",
    "        (points[:, 1] < TARGET_COORDS_2D[1] + search_radius)\n",
    "    )\n",
    "    points_near_target = points[points_near_target_mask]\n",
    "\n",
    "    if len(points_near_target) > 0:\n",
    "        # Find the highest Z value in the vicinity of the target\n",
    "        target_z = np.max(points_near_target[:, 2]) + 0.01\n",
    "    else:\n",
    "        print(f\"Warning: No LiDAR points found near target coordinates. Using scene maximum Z as a fallback.\")\n",
    "        target_z = scene_max[2]\n",
    "\n",
    "    analysis_point = np.array([TARGET_COORDS_2D[0], TARGET_COORDS_2D[1], target_z])\n",
    "    print(f\"Analysis point set to: {analysis_point}\")\n",
    "\n",
    "    # 4. Define Solar Angles and Run Simulation Loop (SEQUENTIAL)\n",
    "    azimuths = np.linspace(0, 2 * np.pi, AZIMUTH_STEPS, endpoint=False)\n",
    "    elevations = np.linspace(0, np.pi / 2, ELEVATION_STEPS, endpoint=True)\n",
    "    solar_radius_rad = np.deg2rad(SOLAR_ANGULAR_RADIUS_DEG)\n",
    "    \n",
    "    simulation_results = []\n",
    "    \n",
    "    total_directions = len(azimuths) * (len(elevations) - 1)\n",
    "    print(f\"\\n--- Starting SEQUENTIAL Cone-Casting Simulation ---\")\n",
    "    print(f\"Casting {NUM_RAYS_PER_CONE} rays per cone for each of {total_directions} solar positions...\")\n",
    "    \n",
    "    current_direction = 0\n",
    "    for el in elevations:\n",
    "        if el < 0.001: continue\n",
    "        \n",
    "        for az in azimuths:\n",
    "            current_direction += 1\n",
    "            if current_direction % 500 == 0:\n",
    "                 print(f\"Processing direction {current_direction} of {total_directions} (Az: {np.rad2deg(az):.1f}째, El: {np.rad2deg(el):.1f}째)...\")\n",
    "            \n",
    "            center_ray_direction = np.array([np.cos(el) * np.sin(az), np.cos(el) * np.cos(az), np.sin(el)])\n",
    "            \n",
    "            cone_ray_vectors = generate_cone_vectors(center_ray_direction, solar_radius_rad, NUM_RAYS_PER_CONE)\n",
    "            \n",
    "            cone_transmittances = []\n",
    "            for ray_vec in cone_ray_vectors:\n",
    "                voxel_path_gen = trace_ray_fast(analysis_point, ray_vec, scene_min, voxel_size, grid_dims)\n",
    "                transmittance = calculate_transmittance(\n",
    "                    voxel_path_gen, classification_grid, density_grid, voxel_size, VEGETATION_EXTINCTION_COEFFICIENTS\n",
    "                )\n",
    "                cone_transmittances.append(transmittance)\n",
    "            \n",
    "            avg_transmittance = np.mean(cone_transmittances)\n",
    "            \n",
    "            simulation_results.append({\n",
    "                'azimuth': az, 'elevation': el, 'transmittance': avg_transmittance\n",
    "            })\n",
    "\n",
    "    # 5. Format and Save Final Attenuation Matrix\n",
    "    print(\"\\n--- Aggregating Results into CSV Matrix ---\")\n",
    "    if not simulation_results:\n",
    "        print(\"No results to save. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "    df = pd.DataFrame(simulation_results)\n",
    "    df['azimuth_deg'] = np.round(np.rad2deg(df['azimuth'])).astype(int)\n",
    "    df['elevation_deg'] = np.round(np.rad2deg(df['elevation'])).astype(int)\n",
    "    \n",
    "    shadow_matrix_df = df.pivot_table(\n",
    "        index='elevation_deg', columns='azimuth_deg', values='transmittance'\n",
    "    )\n",
    "    \n",
    "    # Format headers to match the example output\n",
    "    shadow_matrix_df.index = [f\"Altitude_{i}\" for i in shadow_matrix_df.index]\n",
    "    shadow_matrix_df.columns = [f\"Azimuth_{c}\" for c in shadow_matrix_df.columns]\n",
    "    \n",
    "    # Sort the DataFrame by numeric index and columns to ensure correct order\n",
    "    shadow_matrix_df = shadow_matrix_df.sort_index(key=lambda x: np.array([int(i.split('_')[1]) for i in x]))\n",
    "    shadow_matrix_df = shadow_matrix_df.sort_index(axis=1, key=lambda x: np.array([int(i.split('_')[1]) for i in x]))\n",
    "\n",
    "    shadow_matrix_df = 1- shadow_matrix_df  # Convert to shadow attenuation (1 - transmittance)\n",
    "\n",
    "    if not os.path.exists(OUTPUT_DIRECTORY):\n",
    "        os.makedirs(OUTPUT_DIRECTORY)\n",
    "        \n",
    "    output_path = os.path.join(OUTPUT_DIRECTORY, OUTPUT_FILENAME)\n",
    "    # Write to CSV with header and index\n",
    "    shadow_matrix_df.to_csv(output_path, header=True, index=True)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"\\n--- Simulation Finished ---\")\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Saved shadow attenuation matrix to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shadow_model.visual_utils import *\n",
    "\n",
    "altitude_range = np.arange(0, 90, 1)\n",
    "azimuth_range = np.arange(0, 360, 1)\n",
    "\n",
    "plot_shadow_polar(shadow_matrix_df.values, altitude_range, azimuth_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(OUTPUT_DIRECTORY):\n",
    "        os.makedirs(OUTPUT_DIRECTORY)\n",
    "        \n",
    "output_path = os.path.join(OUTPUT_DIRECTORY, \"shadow_matrix_re.csv\")\n",
    "# Write to CSV without header or index\n",
    "shadow_matrix_df = 1- shadow_matrix_df\n",
    "shadow_matrix_df.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "shadow_matrix_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
